{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks\n",
    "import plotly.express as px\n",
    "from IPython.display import Markdown, display\n",
    "import util.performance_metrics\n",
    "\n",
    "cufflinks.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time range\n",
    "start_time = datetime(2020,1,1, tzinfo=timezone.utc)\n",
    "end_time = datetime(2020, 10, 26, tzinfo=timezone.utc)\n",
    "\n",
    "# Set time horizon for forecasts (should match the forecast horizon from the backtest)\n",
    "known_ahead_targets = [timedelta(hours=24)]\n",
    "\n",
    "# Select forecast percentile\n",
    "percentile = 10\n",
    "quantile_column_name = \"quantile_P10\"\n",
    "\n",
    "# Set case name\n",
    "case_name = \"sun_heavy\"\n",
    "\n",
    "# Scale congestion limit to more reasonable limit (I don't know which one is used where)\n",
    "limit_scale = 0.9\n",
    "limit = -2.5\n",
    "\n",
    "# Evaluation window for metrics\n",
    "eval_window = 14\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pd.read_pickle(\"data/forecast_raycast.pkl\")[quantile_column_name]\n",
    "measurement = pd.read_pickle(\"data/forecast_raycast.pkl\")[\"realised\"]\n",
    "\n",
    "# Slice forecast and measurement on start_time and end_time\n",
    "forecast = forecast[start_time:end_time]\n",
    "measurement = measurement[start_time:end_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(pd.concat([measurement, forecast], axis=1), title=f'Forecast and measurement in {case_name}<br><sup>Forecast percentile = {percentile}</sup>')\n",
    "fig.add_hline(y=limit * limit_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance for entire date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _, number_peak_days, _ = util.performance_metrics.get_performance_metrics(measurement, forecast, limit * limit_scale)\n",
    "display(Markdown(f\"There were <b>{number_peak_days} days</b> with measured peaks using <b>{limit_scale * 100}%</b> of the technical limit.\"))\n",
    "display(Markdown(f\"<b>{round(precision * 100, 1)}% of predicted peaks correspond to actual peaks</b>. Precision is {precision}\"))\n",
    "display(Markdown(f\"<b>{round(recall * 100, 1)}% of actual peaks are predicted</b>. Recall is {recall}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_days = [start_time + timedelta(days=x) for x in range((end_time - start_time).days - eval_window)]\n",
    "performance_metrics_list = []\n",
    "for start_day in start_days:\n",
    "    measurement_window = measurement[start_day:start_day+timedelta(days=eval_window)]\n",
    "    forecast_window = forecast[start_day:start_day+timedelta(days=eval_window)]\n",
    "    metrics = util.performance_metrics.get_performance_metrics(measurement_window, forecast_window, limit * limit_scale)\n",
    "    performance_metrics_list.append(metrics)\n",
    "\n",
    "\n",
    "performance_metrics = pd.DataFrame(performance_metrics_list, columns=[\"precision\", \"recall\", \"F10\", \"number of peaks\", \"number of predicted peaks\"])\n",
    "performance_metrics.index = [start_day + timedelta(days=eval_window) for start_day in start_days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_plot = px.bar(performance_metrics[[\"precision\", \"recall\"]],\n",
    "                               barmode='group',\n",
    "                               title=f\"Precision and recall of peaks in the previous {eval_window} days in {case_name}<br><sup>Forecast percentile = {percentile}</sup><br><sup>T-ahead = {known_ahead_targets[0].total_seconds() // 3600} hours</sup><br><sup>Peak = exceeding {limit_scale * 100}% of the congestion limit</sup>\")\n",
    "f10_plot = px.bar(performance_metrics[[\"F10\"]],\n",
    "                  text=performance_metrics[\"F10\"].apply(lambda x: np.round(x, 3)),\n",
    "                  title=f\"F10 score in the previous {eval_window} days in {case_name}<br><sup>Forecast percentile = {percentile}</sup><br><sup>T-ahead = {known_ahead_targets[0].total_seconds() // 3600} hours</sup><br><sup>Peak = exceeding {limit_scale * 100}% of the congestion limit</sup>\")\n",
    "number_peaks_plot = px.bar(performance_metrics[[\"number of peaks\", \"number of predicted peaks\"]],\n",
    "                           barmode='group',\n",
    "                           title=f\"Number of peaks in the previous {eval_window} days in {case_name}<br><sup>Forecast percentile = {percentile}</sup><br><sup>T-ahead = {known_ahead_targets[0].total_seconds() // 3600} hours</sup><br><sup>Peak = exceeding {limit_scale * 100}% of the congestion limit</sup>\")\n",
    "precision_recall_plot.show()\n",
    "f10_plot.show()\n",
    "number_peaks_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icarus-analyses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
